{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from jax import numpy as jnp, random, vmap, value_and_grad, jit\n", "from pytket import Circuit\n", "from pytket.circuit.display import render_circuit_jupyter\n", "from pytket.extensions.qujax import tk_to_qujax\n", "import qujax\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Define the classification task<br>\n", "We'll try and learn a _donut_ binary classification function (i.e. a bivariate coordinate is labelled 1 if it is inside the donut and 0 if it is outside)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["inner_rad = 0.25; outer_rad = 0.75\n", "def classification_function(x, y):\n", "    r = jnp.sqrt(x**2 + y**2)\n", "    return jnp.where((r > inner_rad)*(r < outer_rad), 1, 0)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["linsp = jnp.linspace(-1, 1, 1000)\n", "Z = vmap(lambda x: vmap(lambda y: classification_function(x, y))(linsp))(linsp)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.contourf(linsp, linsp, Z, cmap='Purples');"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now let's generate some data for our quantum circuit to learn from"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_data = 1000\n", "x = random.uniform(random.PRNGKey(0), shape=(n_data, 2), minval=-1, maxval=1)\n", "y = classification_function(x[:,0], x[:, 1])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.scatter(x[:,0], x[:,1], alpha=jnp.where(y, 1, 0.2), s=10);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Quantum circuit time<br>\n", "We'll use a variant of data re-uploading [P\u00e9rez-Salinas et al](https://doi.org/10.22331/q-2020-02-06-226) to encode the input data, alongside some variational parameters within a quantum circuit classifier"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_qubits = 3\n", "depth = 5"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["c = Circuit(n_qubits)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for layer in range(depth):\n", "    for qi in range(n_qubits):\n", "        c.Rz(0., qi)\n", "        c.Ry(0., qi)\n", "        c.Rz(0., qi)\n", "    \n", "    if layer < (depth - 1):\n", "        for qi in range(layer, layer + n_qubits - 1, 2):\n", "            c.CZ(qi % n_qubits, (qi + 1) % n_qubits)\n", "        c.add_barrier(range(n_qubits))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["render_circuit_jupyter(c)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can use `pytket-qujax` to generate our angles-to-statetensor function."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["angles_to_st = tk_to_qujax(c)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We'll parameterise each angle as<br>\n", "$$ \\theta_k = b_k + w_k * x_k $$<br>\n", "where $b_k, w_k$ are variational parameters to be learnt and $x_k = x_0$ if $k$ even, $x_k = x_1$ if $k$ odd for a single bivariate input point $(x_0, x_1)$."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_angles = 3 * n_qubits * depth\n", "n_params = 2 * n_angles"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def param_and_x_to_angles(param, x_single):\n", "    biases = param[:n_angles]\n", "    weights = param[n_angles:]\n", "    \n", "    weights_times_data = jnp.where(jnp.arange(n_angles) % 2 == 0, weights * x_single[0], weights * x_single[1])\n", "    \n", "    angles = biases + weights_times_data\n", "    return angles"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["param_and_x_to_st = lambda param, x_single: angles_to_st(param_and_x_to_angles(param, x_single))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We'll measure the first qubit only (if its 1 we label _donut_, if its 0 we label _not donut_)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def param_and_x_to_probability(param, x_single):\n", "    st = param_and_x_to_st(param, x_single)\n", "    all_probs = jnp.square(jnp.abs(st))\n", "    first_qubit_probs = jnp.sum(all_probs, axis=range(1, n_qubits))\n", "    return first_qubit_probs[1]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The ideal loss function is the log-likelihood<br>\n", "$$ \\log p(y \\mid q_{(b, w)}(x)) = {\\mathbb{I}[y = 0]}\\log(1 - q_{(b, w)}(x)) + {\\mathbb{I}[y = 1]} \\log(q_{(b, w)}(x))$$<br>\n", "where $q_{(b, w)}(x)$ is the probability the quantum circuit classifies input $x$ as donut given variational parameter vectors $(b, w)$. However this cannot be approximated unbiasedly with shots (in qujax simulations we can use the statetensor to calculate this exactly, but it is still good to keep in mind loss functions that can also be used with shots from a quantum device)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Instead we can minimise the expected Hamming distance between shots and data<br>\n", "$$ C(b, w, x, y) = \\mathbb{E}_{y' \\sim p(\\cdot \\mid q_{(b, w)}(x))}[\\ell(y', y)] = (1 - q_{(b, w)}(x)) \\ell(0, y) + q_{(b, w)}(x)\\ell(1, y), $$<br>\n", "where $y'$ are shots, $y$ are the data labels and $\\ell$ is the Hamming distance. The full batch cost function is $C(b, w) = \\frac1N \\sum_{i=1}^N C(b, w, x_i, y_i)$.<br>\n", "Note that to calculate the cost function we need to evaluate the statetensor for every input point $x_i$. If the dataset becomes too large, we can easily minibatch."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def param_to_cost(param):\n", "    donut_probs = vmap(param_and_x_to_probability, in_axes=(None, 0))(param, x)\n", "    costs = jnp.where(y, 1-donut_probs, donut_probs)\n", "    return costs.mean()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Ready to descend some gradients?<br>\n", "We'll just use vanilla gradient descent here"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["param_to_cost_and_grad = jit(value_and_grad(param_to_cost))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_iter = 1000\n", "stepsize = 1e-1\n", "param = random.uniform(random.PRNGKey(1), shape=(n_params,), minval=0, maxval=2)\n", "costs = jnp.zeros(n_iter)\n", "for i in range(n_iter):\n", "    cost, grad = param_to_cost_and_grad(param)\n", "    costs = costs.at[i].set(cost)\n", "    param = param - stepsize * grad\n", "    print(i, 'Cost: ', cost, end='\\r')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(costs)\n", "plt.xlabel('Iteration')\n", "plt.ylabel('Cost');"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Visualise trained classifier"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["linsp = jnp.linspace(-1, 1, 100)\n", "Z = vmap(lambda a: vmap(lambda b: param_and_x_to_probability(param, jnp.array([a, b])))(linsp))(linsp)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.contourf(linsp, linsp, Z, cmap='Purples', alpha=0.8)\n", "circle_linsp = jnp.linspace(0, 2 * jnp.pi, 100)\n", "plt.plot(inner_rad * jnp.cos(circle_linsp), inner_rad * jnp.sin(circle_linsp), c='red')\n", "plt.plot(outer_rad * jnp.cos(circle_linsp), outer_rad * jnp.sin(circle_linsp), c='red');"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Looks good, it has clearly grasped the donut shape. Sincerest apologies if you are now hungry! \ud83c\udf69"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}